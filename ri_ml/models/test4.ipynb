{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21615719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrait_tools as pt\n",
    "import feature_engineering as fe  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776886a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        66\n",
      "          FD       0.58      0.35      0.44       193\n",
      "          FQ       0.36      0.08      0.13        65\n",
      "          GG       0.63      0.46      0.54        41\n",
      "          IR       0.67      0.24      0.35        84\n",
      "          JK       1.00      0.08      0.15        12\n",
      "          NF       0.50      0.02      0.04        47\n",
      "           O       0.00      0.00      0.00         2\n",
      "          OQ       0.98      0.97      0.97       221\n",
      "          PA       0.82      0.86      0.84       365\n",
      "          PF       0.71      0.41      0.52        97\n",
      "          RQ       0.29      0.04      0.07        49\n",
      "\n",
      "   micro avg       0.80      0.55      0.65      1242\n",
      "   macro avg       0.54      0.29      0.34      1242\n",
      "weighted avg       0.69      0.55      0.58      1242\n",
      " samples avg       0.68      0.61      0.63      1242\n",
      "\n",
      "ðŸ“Š Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        46\n",
      "          FD       0.76      0.38      0.51       226\n",
      "          FQ       0.61      0.19      0.29        59\n",
      "          GG       0.64      0.36      0.46        25\n",
      "          IR       0.62      0.23      0.34       103\n",
      "          JK       0.00      0.00      0.00        12\n",
      "          NF       0.00      0.00      0.00        54\n",
      "           O       0.00      0.00      0.00         4\n",
      "          OQ       0.98      0.95      0.96       226\n",
      "          PA       0.81      0.87      0.84       364\n",
      "          PF       0.81      0.40      0.54        87\n",
      "          RQ       1.00      0.12      0.21        51\n",
      "\n",
      "   micro avg       0.83      0.56      0.67      1257\n",
      "   macro avg       0.52      0.29      0.35      1257\n",
      "weighted avg       0.73      0.56      0.60      1257\n",
      " samples avg       0.70      0.61      0.64      1257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾ï¼ˆæ³¨æ„é¡ºåºä¿æŒä¸€è‡´ï¼‰\n",
    "onehot_cols = ['CQ','FD','FQ','GG','IR','JK','NF','O','OQ','PA','PF','RQ']\n",
    "mlb = MultiLabelBinarizer(classes=onehot_cols)\n",
    "\n",
    "def load_dataset(path):\n",
    "    labels = []\n",
    "    features = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                parts = line.strip().split()\n",
    "                raw_labels = parts[0].split('_')  # å¯èƒ½æ˜¯å¤šæ ‡ç­¾ï¼ˆä»¥_è¿žæŽ¥ï¼‰\n",
    "                feat = list(map(float, parts[1:]))\n",
    "                labels.append(raw_labels)\n",
    "                features.append(feat)\n",
    "    # è½¬ä¸º one-hot\n",
    "    y = mlb.fit_transform(labels)\n",
    "    X = np.array(features)\n",
    "    return X, y\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "X_train, y_train = load_dataset(\"../data/msdialog/train_features.tsv\")\n",
    "X_valid, y_valid = load_dataset(\"../data/msdialog/valid_features.tsv\")\n",
    "X_test, y_test   = load_dataset(\"../data/msdialog/test_features.tsv\")\n",
    "\n",
    "# æž„å»ºå¤šæ ‡ç­¾éšæœºæ£®æž—\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf = MultiOutputClassifier(rf)\n",
    "\n",
    "# æ¨¡åž‹è®­ç»ƒ\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# éªŒè¯é›†é¢„æµ‹ä¸ŽæŠ¥å‘Š\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "print(\"ðŸ“Š Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=onehot_cols, zero_division=0))\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹ä¸ŽæŠ¥å‘Š\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"ðŸ“Š Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=onehot_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edb6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        66\n",
      "          FD       0.53      0.34      0.41       193\n",
      "          FQ       0.50      0.12      0.20        65\n",
      "          GG       0.71      0.49      0.58        41\n",
      "          IR       0.54      0.15      0.24        84\n",
      "          JK       0.33      0.08      0.13        12\n",
      "          NF       0.27      0.06      0.10        47\n",
      "           O       0.00      0.00      0.00         2\n",
      "          OQ       0.98      0.97      0.97       221\n",
      "          PA       0.79      0.86      0.82       365\n",
      "          PF       0.69      0.49      0.57        97\n",
      "          RQ       0.57      0.08      0.14        49\n",
      "\n",
      "   micro avg       0.76      0.56      0.64      1242\n",
      "   macro avg       0.49      0.30      0.35      1242\n",
      "weighted avg       0.66      0.56      0.58      1242\n",
      " samples avg       0.67      0.62      0.63      1242\n",
      "\n",
      "ðŸ“Š Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        46\n",
      "          FD       0.73      0.41      0.53       226\n",
      "          FQ       0.71      0.20      0.32        59\n",
      "          GG       0.65      0.52      0.58        25\n",
      "          IR       0.60      0.17      0.27       103\n",
      "          JK       0.25      0.08      0.12        12\n",
      "          NF       0.50      0.04      0.07        54\n",
      "           O       0.00      0.00      0.00         4\n",
      "          OQ       0.98      0.95      0.96       226\n",
      "          PA       0.81      0.88      0.84       364\n",
      "          PF       0.78      0.37      0.50        87\n",
      "          RQ       0.64      0.14      0.23        51\n",
      "\n",
      "   micro avg       0.82      0.57      0.67      1257\n",
      "   macro avg       0.55      0.31      0.37      1257\n",
      "weighted avg       0.74      0.57      0.61      1257\n",
      " samples avg       0.70      0.62      0.64      1257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# æž„é€ æ¨¡åž‹\n",
    "base_estimator = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "clf = MultiOutputClassifier(base_estimator)\n",
    "\n",
    "# æ‹Ÿåˆæ¨¡åž‹\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# é¢„æµ‹å¹¶è¯„ä¼°\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "print(\"ðŸ“Š Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=onehot_cols, zero_division=0))\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"ðŸ“Š Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=onehot_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f230cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        66\n",
      "          FD       0.56      0.31      0.39       193\n",
      "          FQ       0.00      0.00      0.00        65\n",
      "          GG       0.58      0.27      0.37        41\n",
      "          IR       0.43      0.04      0.07        84\n",
      "          JK       0.00      0.00      0.00        12\n",
      "          NF       1.00      0.02      0.04        47\n",
      "           O       0.00      0.00      0.00         2\n",
      "          OQ       0.97      0.97      0.97       221\n",
      "          PA       0.76      0.87      0.81       365\n",
      "          PF       0.82      0.46      0.59        97\n",
      "          RQ       0.00      0.00      0.00        49\n",
      "\n",
      "   micro avg       0.79      0.52      0.63      1242\n",
      "   macro avg       0.43      0.24      0.27      1242\n",
      "weighted avg       0.63      0.52      0.54      1242\n",
      " samples avg       0.65      0.58      0.60      1242\n",
      "\n",
      "ðŸ“Š Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.00      0.00      0.00        46\n",
      "          FD       0.70      0.31      0.43       226\n",
      "          FQ       0.00      0.00      0.00        59\n",
      "          GG       0.67      0.24      0.35        25\n",
      "          IR       0.62      0.05      0.09       103\n",
      "          JK       0.00      0.00      0.00        12\n",
      "          NF       0.00      0.00      0.00        54\n",
      "           O       0.00      0.00      0.00         4\n",
      "          OQ       0.97      0.95      0.96       226\n",
      "          PA       0.77      0.87      0.82       364\n",
      "          PF       0.87      0.38      0.53        87\n",
      "          RQ       0.00      0.00      0.00        51\n",
      "\n",
      "   micro avg       0.82      0.51      0.63      1257\n",
      "   macro avg       0.38      0.23      0.26      1257\n",
      "weighted avg       0.65      0.51      0.54      1257\n",
      " samples avg       0.65      0.57      0.59      1257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# æž„é€  SVM æ¨¡åž‹\n",
    "clf = OneVsRestClassifier(LinearSVC(random_state=42, max_iter=5000))\n",
    "\n",
    "# æ‹Ÿåˆ\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# éªŒè¯é›†é¢„æµ‹\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "print(\"ðŸ“Š Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=onehot_cols, zero_division=0))\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"ðŸ“Š Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=onehot_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea67dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.22      0.33      0.27        66\n",
      "          FD       0.41      0.32      0.36       193\n",
      "          FQ       0.26      0.28      0.27        65\n",
      "          GG       0.15      0.95      0.25        41\n",
      "          IR       0.21      0.74      0.32        84\n",
      "          JK       0.03      1.00      0.05        12\n",
      "          NF       0.16      0.32      0.21        47\n",
      "           O       0.01      1.00      0.01         2\n",
      "          OQ       0.93      0.97      0.95       221\n",
      "          PA       0.69      0.87      0.77       365\n",
      "          PF       0.32      0.81      0.46        97\n",
      "          RQ       0.24      0.59      0.35        49\n",
      "\n",
      "   micro avg       0.31      0.70      0.43      1242\n",
      "   macro avg       0.30      0.68      0.36      1242\n",
      "weighted avg       0.52      0.70      0.57      1242\n",
      " samples avg       0.43      0.74      0.50      1242\n",
      "\n",
      "ðŸ“Š Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CQ       0.20      0.50      0.29        46\n",
      "          FD       0.38      0.30      0.34       226\n",
      "          FQ       0.23      0.32      0.27        59\n",
      "          GG       0.10      0.96      0.19        25\n",
      "          IR       0.26      0.70      0.38       103\n",
      "          JK       0.02      0.83      0.05        12\n",
      "          NF       0.16      0.31      0.21        54\n",
      "           O       0.01      0.50      0.01         4\n",
      "          OQ       0.92      0.95      0.93       226\n",
      "          PA       0.69      0.85      0.76       364\n",
      "          PF       0.29      0.75      0.41        87\n",
      "          RQ       0.20      0.49      0.28        51\n",
      "\n",
      "   micro avg       0.31      0.67      0.42      1257\n",
      "   macro avg       0.29      0.62      0.34      1257\n",
      "weighted avg       0.51      0.67      0.56      1257\n",
      " samples avg       0.43      0.71      0.49      1257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ä½¿ç”¨ GaussianNBï¼ˆé€‚åˆè¿žç»­ç‰¹å¾ï¼‰\n",
    "clf = OneVsRestClassifier(GaussianNB())\n",
    "\n",
    "# æ‹Ÿåˆæ¨¡åž‹\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# éªŒè¯é›†é¢„æµ‹\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "print(\"ðŸ“Š Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=onehot_cols, zero_division=0))\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"ðŸ“Š Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=onehot_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344c07d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NearestNeighbors.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m classifier \u001b[38;5;241m=\u001b[39m MLkNN(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# æ‹Ÿåˆ\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# éªŒè¯é›†é¢„æµ‹\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_valid_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_valid)\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/skmultilearn/adapt/mlknn.py:218\u001b[0m, in \u001b[0;36mMLkNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_prior(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_cache)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Computing the posterior probabilities\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_true, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond_prob_false \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_cond(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_cache)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/skmultilearn/adapt/mlknn.py:165\u001b[0m, in \u001b[0;36mMLkNN._compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_cond\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to compute for the posterior probabilities\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m        the posterior probability given false\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_ \u001b[38;5;241m=\u001b[39m NearestNeighbors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    166\u001b[0m     c \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    167\u001b[0m     cn \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: NearestNeighbors.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class MyMultiLabelKNN:\n",
    "    def __init__(self, k=10, threshold=0.5):\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.k)\n",
    "        self.nn.fit(X)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        distances, indices = self.nn.kneighbors(X_test)\n",
    "        y_pred = []\n",
    "\n",
    "        for neighbors in indices:\n",
    "            # å–è¿™äº›é‚»å±…çš„æ ‡ç­¾\n",
    "            neighbor_labels = self.y_train[neighbors]\n",
    "            # æŒ‰åˆ—æ±‚å’Œï¼ˆæ¯ä¸ªæ ‡ç­¾å‡ºçŽ°çš„æ¬¡æ•°ï¼‰\n",
    "            label_counts = np.sum(neighbor_labels, axis=0)\n",
    "            # é˜ˆå€¼å†³å®šæ˜¯å¦æ¿€æ´»è¯¥æ ‡ç­¾\n",
    "            label_pred = (label_counts / self.k) >= self.threshold\n",
    "            y_pred.append(label_pred.astype(int))\n",
    "\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "# è®­ç»ƒå’Œé¢„æµ‹\n",
    "clf = MyMultiLabelKNN(k=10, threshold=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "print(\"ðŸ“Š Validation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=onehot_cols, zero_division=0))\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"ðŸ“Š Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=onehot_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20f789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
