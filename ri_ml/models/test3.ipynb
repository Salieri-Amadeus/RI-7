{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf935ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/langlang056/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/langlang056/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pretrait_tools as pt\n",
    "import feature_engineering as fe  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')          # 单词/句子分词\n",
    "nltk.download('vader_lexicon')  # 情感分析用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4011a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mpqa_subj_lexicon.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeature_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m        \u001b[38;5;66;03m# 你的其它函数照旧\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# ---- 修正④：加载 MPQA 词典，一次即可 ----\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m mpqa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmpqa_subj_lexicon.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     45\u001b[0m                    names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpol\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     46\u001b[0m pos_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mpqa\u001b[38;5;241m.\u001b[39mloc[mpqa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     47\u001b[0m neg_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(mpqa\u001b[38;5;241m.\u001b[39mloc[mpqa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mpqa_subj_lexicon.txt'"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ➊ 读取 & 扁平化  （保持不变）\n",
    "# =========================================================\n",
    "INPUT_FILE = \"../data/all.tsv\"\n",
    "dialogs = pt.load_dialogs(INPUT_FILE)\n",
    "df = pt.flatten_dialogs(dialogs)\n",
    "df, mlb = pt.binarize_labels(df)      # 现在 df 里就有 'CQ' 'FD' … 列\n",
    "\n",
    "# =========================================================\n",
    "# ➋ 划分 Train / Test *在这一段加入修正① & ②*\n",
    "# =========================================================\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=df['dialog_id']))\n",
    "train_df, test_df   = df.iloc[train_idx].copy(), df.iloc[test_idx].copy()\n",
    "\n",
    "# —— 修正②：Label-Powerset 变单标签（如果你想保持 one-hot 可跳过）——\n",
    "onehot_cols = ['CQ','FD','FQ','GG','IR','JK','NF','O',\n",
    "               'OQ','PA','PF','RQ']\n",
    "def to_lp(row):\n",
    "    return '&'.join([lab for lab in onehot_cols if row[lab]==1]) or '__NONE__'\n",
    "\n",
    "train_df['lp_label'] = train_df.apply(to_lp, axis=1)\n",
    "test_df ['lp_label'] = test_df .apply(to_lp, axis=1)\n",
    "\n",
    "# =========================================================\n",
    "# ➌ 训练 TF-IDF  *在这一段加入修正①*\n",
    "# =========================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize(x):           # 论文里就用简单分词即可\n",
    "    return nltk.word_tokenize(x.lower())\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer=tokenize, lowercase=False, min_df=2)\n",
    "vec.fit(train_df['text'])          # ← 只用训练集拟合\n",
    "\n",
    "# =========================================================\n",
    "# ➍ 提特征  *把修正③ ④ 放进 extract_dialog_features() 里*\n",
    "# =========================================================\n",
    "from feature_utils import *        # 你的其它函数照旧\n",
    "\n",
    "# ---- 修正④：加载 MPQA 词典，一次即可 ----\n",
    "mpqa = pd.read_csv('mpqa_subj_lexicon.txt', sep='\\t',\n",
    "                   names=['word','pos','pol'])\n",
    "pos_dict = set(mpqa.loc[mpqa['pol']=='positive','word'])\n",
    "neg_dict = set(mpqa.loc[mpqa['pol']=='negative','word'])\n",
    "\n",
    "# ---- 修正③：改进 duplicate() 写在函数内部 ----\n",
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "def canon_sent(s):\n",
    "    toks = [stemmer.stem(t) for t in tokenize(s) if t not in stop]\n",
    "    return ' '.join(toks)\n",
    "\n",
    "# ========= 定义特征函数 =========\n",
    "def extract_dialog_features(dialog):\n",
    "    \"\"\"dialog 是同一 dialog_id 的 DataFrame（已按 turn_id 排序）\"\"\"\n",
    "    utters = dialog['text'].tolist()\n",
    "    tfidf = vec.transform(utters)\n",
    "    first_vec = tfidf[0]\n",
    "    \n",
    "    init_sim  = cosine_similarity(tfidf, first_vec).ravel()\n",
    "    thread_sim = []\n",
    "    mean_vec = first_vec.copy()\n",
    "    for i in range(len(utters)):\n",
    "        if i == 0:\n",
    "            thread_sim.append(1.0)\n",
    "        else:\n",
    "            thread_sim.append(cosine_similarity(tfidf[i], mean_vec/i).item())\n",
    "            mean_vec += tfidf[i]\n",
    "    \n",
    "    qm   = [\"?\" in u for u in utters]\n",
    "    dup  = pd.Series(utters).str.lower().duplicated().tolist()\n",
    "    \n",
    "    w_pattern = re.compile(r'\\b(who|what|when|where|why|how)\\b', re.I)\n",
    "    w5h1 = [{w: int(bool(re.search(fr'\\b{w}\\b', u, re.I))) for w in\n",
    "             [\"who\",\"what\",\"when\",\"where\",\"why\",\"how\"]} for u in utters]\n",
    "    \n",
    "    abs_pos  = np.arange(1, len(utters)+1)\n",
    "    norm_pos = abs_pos / len(utters)\n",
    "    \n",
    "    tokens   = [tokenize(u) for u in utters]\n",
    "    post_len = [len(t) for t in tokens]\n",
    "    uniq_len = [len(set(t)) for t in tokens]\n",
    "    uniq_stm = [len({stemmer.stem(w) for w in t}) for t in tokens]\n",
    "    \n",
    "    thx  = [bool(re.search(r'\\bthanks?\\b', u, re.I)) for u in utters]\n",
    "    excl = [\"!\" in u for u in utters]\n",
    "    vef  = [bool(re.search(r'\\b(very|extremely)\\s+(good|helpful|useful|nice|excellent)\\b', u, re.I))\n",
    "            for u in utters]\n",
    "    \n",
    "    vader = [sia.polarity_scores(u) for u in utters]\n",
    "    \n",
    "    pos_cnt = [sum(w in pos_dict for w in t) for t in tokens]\n",
    "    neg_cnt = [sum(w in neg_dict for w in t) for t in tokens]\n",
    "    \n",
    "    # --- 写回原 dialog DataFrame ---\n",
    "    dialog = dialog.assign(\n",
    "        init_sim=init_sim, thread_sim=thread_sim,\n",
    "        qm=qm, dup=dup,\n",
    "        abs_pos=abs_pos, norm_pos=norm_pos,\n",
    "        post_len=post_len, uniq_len=uniq_len, uniq_stm=uniq_stm,\n",
    "        thank=thx, exclam=excl, ve_feedback=vef,\n",
    "        pos_score=[v['pos'] for v in vader],\n",
    "        neg_score=[v['neg'] for v in vader],\n",
    "        neu_score=[v['neu'] for v in vader],\n",
    "        comp_score=[v['compound'] for v in vader],\n",
    "        pos_cnt=pos_cnt, neg_cnt=neg_cnt,\n",
    "        **{f'{w}_flag':[d[w] for d in w5h1] for w in [\"who\",\"what\",\"when\",\"where\",\"why\",\"how\"]}\n",
    "    )\n",
    "    return dialog\n",
    "\n",
    "# —— 在 train_df / test_df 上分别提特征 —— \n",
    "train_df = (train_df.sort_values(['dialog_id','turn_id'])\n",
    "                    .groupby('dialog_id', group_keys=False)\n",
    "                    .apply(extract_dialog_features))\n",
    "\n",
    "test_df  = (test_df.sort_values(['dialog_id','turn_id'])\n",
    "                    .groupby('dialog_id', group_keys=False)\n",
    "                    .apply(extract_dialog_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceee944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
